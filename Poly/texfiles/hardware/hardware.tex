\begin{savequote}[65mm]
--- A novice was trying to fix a broken Lisp machine by turning the power off and on. Knight, seeing what the student was doing spoke sternly: "You cannot fix a machine just by power-cycling it with no understanding of what is going wrong." Knight turned the machine off and on. The machine worked.
\qauthor{In \cite{AMP}, from "Al Koans", a collection of jokes popular at MIT in the 1980s}
\end{savequote}

\chapter{Quelques notions hardware}
\label{chapter:hardware}

\begin{center}
\textit{Ce chapitre peut être omis en première lecture.}
\end{center}

Ce chapitre a pour vocation de fournir une très brève introduction aux différents composants d'un ordinateur commercial. Au fur et à mesure des progrès de l'informatique, le développement logiciel a été suffisamment abstrait du hardware sur lequel les programmes étaient exécutés pour qu'il soit possible de développer une application sans connaissances particulières sur les machines exécutant le code. Cependant, les applications intensives en "calcul" impliquent une utilisation efficace du hardware qui procède directement des connaissances du développeur; les abstractions développées se révélant souvent insuffisantes pour gérer de manière performante le hardware dans les cas critiques.\\

Depuis 30 ans, les progrès technologiques en terme de hardware ont été tels qu'il faut donc désormais des années pour acquérir une connaissance satisfaisante sur le fonctionnement d'un simple ordinateur classique. C'est bien évidemment très au-delà de l'ambition de ce cours, mais certains points seront abordés dans le cours avancé dispensé en troisième année. Nous renvoyons le lecteur curieux à une introduction en la matière très technique mais accessible aux plus motivés : \cite{Drepper}. Dans ce chapitre, nous ne donnons donc qu'un aperçu des différents composants d'une machine classique afin de donner quelques idées élémentaires.\\

On peut définir la naissance des premiers ordinateurs (plutôt calculatrices géantes) dans les années 40, avec l'équipe Von Neumann. Pour les besoins en calculs du projet Manhattan à Los Alamos, Von Neumann élabore avec Mauchly et Eckert en s'inspirant des travaux de Turing\footnote{Alan Turing est avec Von Neumann l'un des pères de l'informatique. Il s'est suicidé en croquant une pomme empoisonnée au cyanure, probablement en référence à Blanche-Neige; le logo d'Apple est un hommage à Turing.} un premier calculateur, dont le design est depuis appelé (plutôt injustement) architecture de Von Neumann. C'est cette architecture qui reste présente dans la quasi-totalité des architectures actuelles \footnote{même si l'architecture interne des micro-processeurs depuis 10 ans s'en écarte continuellement}. Dans ces premiers ordinateurs, la machine avait très principalement vocation au calcul. Chacun des composants était fabriqué spécifiquement pour cette machine, par le même constructeur.\\

Ce n'est plus aujourd'hui le cas : les différents constructeurs de CPU, de RAM ou de cartes graphiques sont des sociétés distinctes. Pour donner quelques exemples, les constructeurs CPU comptent des sociétés comme Intel, AMD, etc.; les constructeurs RAM sont représentés par exemple par Samsung, Toshiba, Rambus, etc.; les constructeurs de carte graphique sont principalement Nvidia et ATI. Le résultat de cette spécialisation des constructeurs dans certains composants est qu'à la différence des premiers ordinateurs, certains des composants d'un ordinateur vont être très rapides par rapport aux autres, et ce seront les composants les plus lents qui ralentiront souvent la machine.\\

\section{Le CPU}

Le CPU (Central Processing Unit) est le composant principal de calcul. Les CPU qui nous intéressent sont construits sur un seul circuit imprimé (depuis les années 1970) : on les appelle micro-processeurs. Un CPU moderne est constitué de millions de transistors, qui permettent de réaliser de très nombreuses opérations mathématiques par seconde.\\

Depuis une dizaine d'années, les processeurs sont progressivement devenus multi-coeurs, c'est à dire qu'ils contiennent chacun plusieurs entités de calcul appelées coeurs. L'organisation et la répartition des calculs sur ces différents coeurs est laissée à la charge des développeurs et du système d'exploitation (OS, pour Operating System).\\

Sur les CPUs modernes\footnote{hormi partiellement pour certaines consoles de jeux}, les différents composants internes de chaque coeur sont synchronisés sur une sorte d'horloge. A chaque modification de l'état interne du coeur, chacun des composants peut se retrouver dans un état électrique différent, et il importe que chacun de ces composants internes soit stabilisé (au sens électrique du terme) avant de "regarder" à nouveau le coeur. L'horloge fournit un mécanisme de synchronisation en fournissant des "ticks" réguliers. A chaque tick, chacun des composants a eu le temps de se stabiliser (fin du régime transitoire) et le coeur se trouve donc dans un état global cohérent. On voit donc que la performance d'un coeur va dépendre de la durée du régime transitoire entre deux états, mesurée par la durée entre deux ticks de l'horloge. Cette performance est donc mesurée en une fréquence, qui décrit le nombre de cycles d'horloge par seconde que peut réaliser le coeur. Actuellement, les processeurs modernes ont des coeurs qui tournent autour de 3 GHz, c'est à dire que chaque coeur réalise un peu plus de 3 milliards de cycles d'horloge par seconde. \\

Les composants internes principaux d'un CPU sont :

\begin{enumerate}
\item \textit{l'unité arithmétique et logique (UAL, ou ALU en anglais)}, qui prend en charge les calculs arithmétiques et les tests logiques (comme les branchements IF).
\item \textit{les registres}, qui sont des mémoires de très petite taille (quelques octets) et en très petit nombre. Ils sont cependant très rapides, et l'UAL peut les manipuler comme elle l'entend à chaque cycle d'horloge.
\item \textit{L'horloge}, qui fournit un mécanisme de synchronisation interne des différents composants.
\item \textit{L'unité de contrôle}, qui utilise l'impulsion fournie par l'horloge pour synchroniser les différents composants.
\item \textit{L'unité d'entrée-sortie}, qui gère les communications avec les périphériques, extérieurs au CPU (RAM, carte graphique, écran, souris, clavier, disque dur, imprimante, ports USB, etc.).
\end{enumerate}

Sur chaque coeur, nous pouvons faire une opération (multiplication ou addition) par cycle d'horloge\footnote{Sur les processeurs hyper-threadés, on peut en fait faire une multiplication ET une addition par cycle d'horloge}, c'est à dire jusqu'à près de 3 milliards d'opérations sur des nombres réels par seconde (3 GFlops).\\

\section{La mémoire}

La mémoire d'un ordinateur est répartie sur de multiples composants. Tous les composants d'un ordinateur stockent des données, au moins celles nécessaires à leur fonctionnement instantané. Certains composants (mémoire RAM, mémoire Cache, disque-dur) sont cependant entièrement dédiés à stocker les données et à les rendre disponibles pour les autres éléments. Ces composants de stockage se distinguent suivant trois critères : la persistence ou non des données lorsque la machine est éteinte/rallumée, la vitesse d'accès aux données stockées, et la capacité de stockage.\\

\subsection{Disque dur (Hard disk drive : HDD)}

Le disque dur est un système de stockage massif de données mais au débit relativement faible. Un disque dur commercial peut actuellement stocker plusieurs Téraoctet (To), c'est à dire mille Gigaoctets (Go). Le disque dur fournit un système de stockage dans lequel les données sont persistées après l'extinction de la machine. C'est sur le disque dur que sont stockées toutes vos données personnelles, les données de vos programmes, les données du système d'explotation, etc.\\

Le principal problème actuel des disques durs est la vitesse avec laquelle les données peuvent être lues et écrites. En effet, les disques-durs actuels\footnote{La techno est en train d'être remplacée par des SSD mais devrait être encore dominante pour quelques années.} reposent sur un système de stockage magnétique sur des disques empilés les uns sur les autres. Tout comme pour les vieux lecteurs de disque 33 et 45 tours, des têtes de lecture se déplacent autour du disque pour aller lire les données\footnote{c'est souvent un des composants les plus fragiles: lorsque vous faites tomber votre laptop, les têtes de lecture viennent souvent perforer les disques, détruisant ainsi le disque dur.}. En raison de problèmes de frottement d'air, les disques durs actuels ne peuvent faire beaucoup mieux que 7200 tours/minute. Cette restriction physique limite de deux manières la performance de lecture/écriture du disque-dur: tout d'abord, le débit d'un disque dur est très faible par rapport aux autres composants de stockage de mémoire; ensuite, la latence entre la requête d'une donnée et l'obtention de cette donnée est élevée, du fait de la nécessité pour la tête de lecture de physiquement se déplacer pour accéder à l'emplacement de la donnée.\\

En conditions réelles, on dépasse très rarement les 100 Mo/sec pour les disques durs internes et les 40 Mo/sec pour les disques durs externes (c'est à dire extérieurs à l'ordinateur et juste reliés par un USB ou un FireWire). Pour la latence c'est encore pire, transférer un octet depuis le disque-dur jusque le CPU met souvent entre 3 et 12 ms, soit plusieurs dizaines de millions de cycles d'horloge !

\subsection{La mémoire RAM}

La mémoire RAM (Random Access Memory) désigne un système de stockage dans lequel l'accès à une donnée est indépendant de l'endroit où la donnée est stockée (contrairement au disque dur où la position de la donnée par rapport aux têtes de lecture modifie sensiblement le temps de lecture/écriture). En première approximation, la mémoire RAM d'un ordinateur est effectivement en temps constant\footnote{Pour être tout à fait rigoureux, ceci n'est pas tout à fait exact dans le cas de la mémoire DRAM qui est communément utilisée comme mémoire RAM dans les ordinateurs récents. Cependant, on peut tout à fait omettre cette remarque en première approximation.}.\\

La mémoire RAM d'un ordinateur n'est pas persistante, c'est à dire qu'elle perd ses données lorsque l'ordinateur est éteint (pensez à tous ces documents que vous avez perdus parce que vous aviez oublié d'enregistrer...). Plusieurs technologies permettent de créer de la mémoire RAM, les plus connues étant la DRAM (dynamic RAM) et la SRAM (static RAM). La SRAM est beaucoup plus performante que la DRAM, mais elle coûte beaucoup plus cher à fabriquer et est donc principalement réservée à la mémoire Cache détaillée dans le paragraphe suivant.\\

La mémoire RAM DDR utilisée dans la plupart de nos ordinateurs permet de stocker moins de données qu'un disque-dur : environ 4Go. Cependant, la mémoire RAM est nettement plus rapide (environ 7Go /sec), et elle peut être accédée par le microprocesseur en quelques centaines de cycles d'horloge.\\

\subsection{La mémoire Cache}

Parfois aussi appelée antémémoire, c'est la roll's de la mémoire d'un ordinateur : très coûteuse et très performante, elle permet de rendre disponibles des données en quelques cycles d'horloge seulement. Généralement, la mémoire Cache est séparée en plusieurs niveaux (L1, L2, L3) de plus en plus volumineux mais de moins en moins rapides. Typiquement, le cache L1 contient quelques kilo-octets et peut être accédé en 2 ou 3 cycles d'horloge. Le cache L3, le moins rapide, contient jusque quelques Mo et peut être accédé en une cinquantaine de cycles d'horloge.

\subsection{Memory swapping}

L'OS est en charge de la gestion de la mémoire, c'est lui qui alloue la mémoire nécessaire à chaque programme. Par défaut, la mémoire nécessaire à l'exécution d'un programme est stockée dans la mémoire RAM et dans les caches. Lorsque cette mémoire vient à manquer (par exemple lors d'un monte carlo mal codé sur un très grand échantillon), diverses solutions peuvent être envisagées suivant l'architecture matérielle. Dans de nombreux systèmes, le dépassement de cette limite entraîne une erreur. Dans d'autres cas, l'OS va échanger (swap) la mémoire RAM manquante avec une partie du disque dur pour simuler la mémoire manquante. Ceci n'est en aucun cas une solution satisfaisante, étant donné que le débit en lecture/écriture d'un disque dur par rapport à de la mémoire DRAM est inférieur à un facteur 1/10. Devoir recourir à du memory swapping est donc souvent signe de performances dégradées d'un facteur 10 ou 100. Ceci se ressent lorsque vous essayez d'exécuter un programme sur des données trop volumineuses: votre programme se met à ralentir très sensiblement, vous pouvez entendre le bruit du disque dur qui tourne à plein régime, et votre programme meurt souvent dans d'horribles souffrances.

\subsection{Prefetching}

Les sections précédentes ont démontré les hiérarchies de mémoire, de la plus rapide à la plus lente : Cache L1, Cache L2, Cache L3, RAM. Comment un programme utilise-t-il ces différents espaces de stockage ? Toutes les données nécessaires à l'exécution d'un programme se trouvent dans la mémoire RAM. La mémoire Cache va servir de copie de la mémoire RAM et devenir une mémoire-tampon; afin de limiter le coût d'accès à la mémoire RAM, la mémoire Cache va copier une partie des données nécessaires à l'exécution du programme. Lorsque la donnée est répliquée en cache, le CPU requêtera la mémoire Cache plutôt que la mémoire RAM, afin d'obtenir la donnée plus rapidement. Lorsque le CPU requête une donnée et que celle-ci se trouve dans le Cache, on parle de Cache Hit; dans le cas contraire, on parle de Cache Miss.\\

Le jeu revient donc à choisir astucieusement quelles sont les données de la RAM qui doivent être copiées en Cache, et dans quel Cache (L1, L2, L3) afin de maximiser le pourcentage des Cache Hit et minimiser ainsi celui des Cache Miss. Ce jeu, appelé Préfetching, n'est heureusement pas la tâche du développeur mais de l'OS et du hardware. En réalité, l'OS connait la liste des prochaines instructions qui seront exécutées\footnote{sauf lorsqu'il y a des instructions conditionnelles comme IF, auquel cas le préfetcheur va faire du branching...} et peut donc anticiper quelles données seront nécessaires dans un futur proche, et s'il est nécessaire de ramener cette donnée à un endroit plus proche du CPU (RAM vers L3, RAM vers L2, RAM vers L1, L3 vers L2, L3 vers L1 ou L2 vers L1).\\

Dans un programme idéal et bien codé (comme une multiplication matricielle par bloc ou un K-Means), une majorité des données nécessaires se trouvent dans le Cache L1 ou L2 au moment où elles doivent être lues ou écrites, c'est à dire que le CPU fait surtout des Cache Hit et celà entraine très peu de ralentissement du CPU. Dans la plupart des cas cependant, le préfetcheur échoue régulièrement à "deviner" quelles sont les données qui vont être accédées, le CPU connait alors beaucoup de Cache Miss, ce qui nuit sensiblement aux performances du programme : le CPU va passer son temps à attendre des données plutôt qu'à les traiter : on parle alors de "starvation" du CPU.\\

Dans la vie de tous les jours, un développeur ne peut pas faire grand chose pour éviter les Cache Miss car il ne maîtrise pas directement les stratégies de préfetching de la machine. Cependant, une conscience de ces problématiques peut parfois permettre de gagner un facteur 10 ou 100 sur la rapidité d'un programme en gardant une notion de localité des données (nous renvoyons le lecteur intéressé à \cite{Drepper}).\\

\warning Sauf dans des cas extrêmes, il n'est pas nécessaire de se soucier des stratégies de préfetching, comme il n'est pas nécessaire d'avoir conscience de ces différentes mémoires. En effet, il ne faut se soucier de la performance que lorsque vous avez déterminé que vous aviez un problème de performance. S'il ne fallait citer qu'un homme, ce serait Donald Knuth : \textbf{\textit{"Early optimization is the root of all Evil."}}. Cette citation sera le moto de ce cours.\\

\section{Le GPU}

La carte graphique, aussi connue sous le terme GPU (Graphic Processor Unit), est une sorte de processeur dédié. C'est en fait un composant matériel qui héberge en son sein plusieurs centaines de micro-CPU élémentaires et bon-marchés. Ces micro-CPU possèdent beaucoup moins d'instructions que les CPU classiques et ont une cadence plus faible. Cependant, leur nombre fait des GPU des unités de calcul extrêmement puissantes dans certains cas.\\

A l'origine, les GPU étaient spécialisés dans le rendu graphique, notamment pour les jeux vidéos et les logiciels de dessin/retouche graphique. Les constructeurs de cartes graphiques, NVidia en tête, ont fourni depuis quelques années des librairies permettant d'utiliser les GPU pour d'autres utilisations (c.f. par exemple CUDA). Les GPU sont des processeurs extrêmement adaptés lorsqu'ils sont utilisés pour appliquer de manière indépendante une fonction à de multiples valeurs. Dans le cadre des jeux vidéos, à chaque pixel de l'écran est en effet appliqué une même transformation de l'espace. Depuis, d'autres utilisations ont été développées, regroupées sous le terme de GPGPU (General Purpose on Graphic Processor Unit); le pricing par Monte-Carlo est un excellent exemple d'utilisation de GPU pour réaliser des calculs plus rapidement que sur CPU.



